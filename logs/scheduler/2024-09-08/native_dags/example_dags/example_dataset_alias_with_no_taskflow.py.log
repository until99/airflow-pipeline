[2024-09-08T15:27:45.839-0300] {processor.py:186} INFO - Started process (PID=32125) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:27:45.839-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:27:45.840-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:27:45.840-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:27:45.855-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:27:45.869-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:27:45.869-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:27:45.882-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:27:45.882-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:27:45.884-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:27:45.883-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:27:45.884-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:27:45.884-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:27:45.885-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:27:45.885-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:27:45.904-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.069 seconds
[2024-09-08T15:28:40.072-0300] {processor.py:186} INFO - Started process (PID=32289) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:28:40.138-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:28:40.139-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:28:40.139-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:28:40.155-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:28:40.169-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:28:40.169-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:28:40.183-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:28:40.183-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:28:40.184-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:28:40.184-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:28:40.185-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:28:40.185-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:28:40.186-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:28:40.186-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:28:40.206-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.138 seconds
[2024-09-08T15:29:57.374-0300] {processor.py:186} INFO - Started process (PID=32486) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:29:57.375-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:29:57.376-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:29:57.376-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:29:57.391-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:29:57.405-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:29:57.405-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:29:57.421-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:29:57.421-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:29:57.422-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:29:57.422-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:29:57.423-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:29:57.423-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:29:57.423-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:29:57.423-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:29:57.443-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.072 seconds
[2024-09-08T15:31:10.050-0300] {processor.py:186} INFO - Started process (PID=32773) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:31:10.051-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:31:10.052-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:31:10.052-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:31:10.068-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:31:10.081-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:31:10.081-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:31:10.095-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:31:10.095-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:31:10.096-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:31:10.096-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:31:10.097-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:31:10.097-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:31:10.097-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:31:10.097-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:31:10.117-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.070 seconds
[2024-09-08T15:32:23.210-0300] {processor.py:186} INFO - Started process (PID=33016) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:32:23.211-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:32:23.212-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:32:23.211-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:32:23.226-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:32:23.239-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:32:23.239-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:32:23.254-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:32:23.254-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:32:23.256-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:32:23.255-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:32:23.256-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:32:23.256-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:32:23.257-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:32:23.257-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:32:23.276-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.070 seconds
[2024-09-08T15:33:38.086-0300] {processor.py:186} INFO - Started process (PID=33310) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:33:38.087-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:33:38.088-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:33:38.087-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:33:38.105-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:33:38.121-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:33:38.121-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:33:38.139-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:33:38.139-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:33:38.141-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:33:38.141-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:33:38.142-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:33:38.141-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:33:38.142-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:33:38.142-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:33:38.161-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.079 seconds
[2024-09-08T15:34:51.672-0300] {processor.py:186} INFO - Started process (PID=33610) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:34:51.673-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:34:51.674-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:34:51.673-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:34:51.688-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:34:51.701-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:34:51.701-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:34:51.716-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:34:51.716-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:34:51.718-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:34:51.718-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:34:51.719-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:34:51.719-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:34:51.720-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:34:51.720-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:34:51.739-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.071 seconds
[2024-09-08T15:35:59.565-0300] {processor.py:186} INFO - Started process (PID=33924) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:35:59.566-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:35:59.567-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:35:59.567-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:35:59.584-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:35:59.598-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:35:59.598-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:35:59.613-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:35:59.613-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:35:59.615-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:35:59.615-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:35:59.616-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:35:59.615-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:35:59.616-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:35:59.616-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:35:59.635-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.073 seconds
[2024-09-08T15:36:53.924-0300] {processor.py:186} INFO - Started process (PID=34053) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:36:53.927-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:36:53.929-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:36:53.928-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:36:53.944-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:36:53.957-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:36:53.957-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:36:53.971-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:36:53.970-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:36:53.973-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:36:53.973-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:36:53.974-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:36:53.974-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:36:53.975-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:36:53.975-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:36:53.994-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.074 seconds
[2024-09-08T15:38:07.272-0300] {processor.py:186} INFO - Started process (PID=34309) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:38:07.273-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:38:07.274-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:38:07.274-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:38:07.288-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:38:07.301-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:38:07.301-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:38:07.315-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:38:07.315-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:38:07.317-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:38:07.316-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:38:07.317-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:38:07.317-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:38:07.318-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:38:07.318-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:38:07.337-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.069 seconds
[2024-09-08T15:39:04.236-0300] {processor.py:186} INFO - Started process (PID=34663) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:39:04.236-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:39:04.238-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:39:04.238-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:39:04.253-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:39:04.267-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:39:04.267-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:39:04.281-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:39:04.281-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:39:04.283-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:39:04.283-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:39:04.284-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:39:04.284-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:39:04.285-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:39:04.285-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:39:04.305-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.072 seconds
[2024-09-08T15:40:20.082-0300] {processor.py:186} INFO - Started process (PID=35034) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:40:20.083-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:40:20.085-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:40:20.084-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:40:20.100-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:40:20.114-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:40:20.114-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:40:20.208-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:40:20.208-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:40:20.210-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:40:20.209-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:40:20.210-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:40:20.210-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:40:20.211-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:40:20.211-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:40:20.230-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.152 seconds
[2024-09-08T15:41:15.476-0300] {processor.py:186} INFO - Started process (PID=35440) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:41:15.477-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:41:15.478-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:41:15.478-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:41:15.496-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:41:15.512-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:41:15.512-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:41:15.529-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:41:15.529-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:41:15.533-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:41:15.533-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:41:15.534-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:41:15.534-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:41:15.535-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:41:15.535-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:41:15.556-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.085 seconds
[2024-09-08T15:42:28.894-0300] {processor.py:186} INFO - Started process (PID=35608) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:42:28.895-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:42:28.896-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:42:28.896-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:42:28.909-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:42:28.922-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:42:28.922-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:42:28.937-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:42:28.937-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:42:28.938-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:42:28.938-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:42:28.939-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:42:28.939-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:42:28.940-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:42:28.939-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:42:28.960-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.069 seconds
[2024-09-08T15:43:21.542-0300] {processor.py:186} INFO - Started process (PID=35909) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:43:21.543-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:43:21.544-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:43:21.544-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:43:21.559-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:43:21.575-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:43:21.575-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:43:21.591-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:43:21.590-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:43:21.592-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:43:21.592-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:43:21.593-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:43:21.592-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:43:21.593-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:43:21.593-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:43:21.612-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.074 seconds
[2024-09-08T15:44:15.434-0300] {processor.py:186} INFO - Started process (PID=36099) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:44:15.435-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:44:15.436-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:44:15.436-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:44:15.454-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:44:15.471-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:44:15.470-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:44:15.487-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:44:15.487-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:44:15.490-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:44:15.490-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:44:15.491-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:44:15.491-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:44:15.492-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:44:15.492-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:44:15.514-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.083 seconds
[2024-09-08T15:45:06.876-0300] {processor.py:186} INFO - Started process (PID=36214) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:45:06.877-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:45:06.879-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:45:06.879-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:45:06.899-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:45:06.919-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:45:06.918-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:45:06.934-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:45:06.934-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:45:06.936-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:45:06.936-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:45:06.936-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:45:06.936-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:45:06.937-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:45:06.937-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:45:06.959-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.086 seconds
[2024-09-08T15:46:26.415-0300] {processor.py:186} INFO - Started process (PID=36532) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:46:26.415-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:46:26.417-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:46:26.416-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:46:26.431-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:46:26.444-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:46:26.443-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:46:26.457-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:46:26.457-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:46:26.458-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:46:26.458-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:46:26.459-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:46:26.459-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:46:26.459-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:46:26.459-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:46:26.479-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.068 seconds
[2024-09-08T15:47:45.169-0300] {processor.py:186} INFO - Started process (PID=36773) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:47:45.170-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:47:45.172-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:47:45.172-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:47:45.190-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:47:45.206-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:47:45.206-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:47:45.222-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:47:45.222-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:47:45.225-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:47:45.225-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:47:45.225-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:47:45.225-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:47:45.226-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:47:45.226-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:47:45.251-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.087 seconds
[2024-09-08T15:48:42.643-0300] {processor.py:186} INFO - Started process (PID=37129) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:48:42.644-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:48:42.646-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:48:42.646-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:48:42.664-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:48:42.684-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:48:42.684-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:48:42.704-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:48:42.704-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:48:42.707-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:48:42.707-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:48:42.709-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:48:42.709-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:48:42.711-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:48:42.710-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:48:42.790-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.152 seconds
[2024-09-08T15:49:36.412-0300] {processor.py:186} INFO - Started process (PID=37684) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:49:36.413-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:49:36.415-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:49:36.414-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:49:36.431-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:49:36.447-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:49:36.446-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:49:36.462-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:49:36.462-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:49:36.464-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:49:36.463-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:49:36.464-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:49:36.464-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:49:36.465-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:49:36.465-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:49:36.484-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.076 seconds
[2024-09-08T15:50:28.589-0300] {processor.py:186} INFO - Started process (PID=38415) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:50:28.589-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:50:28.591-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:50:28.590-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:50:28.605-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:50:28.679-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:50:28.679-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:50:28.689-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:50:28.689-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T15:50:28.695-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:50:28.695-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:50:28.696-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:50:28.695-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:50:28.698-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:50:28.698-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:50:28.699-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:50:28.699-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:50:28.722-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.137 seconds
[2024-09-08T15:51:22.699-0300] {processor.py:186} INFO - Started process (PID=38938) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:51:22.700-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:51:22.701-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:51:22.701-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:51:22.715-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:51:22.729-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:51:22.729-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:51:22.744-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:51:22.744-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:51:22.745-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:51:22.745-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:51:22.746-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:51:22.746-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:51:22.747-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:51:22.747-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:51:22.766-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.071 seconds
[2024-09-08T15:52:38.737-0300] {processor.py:186} INFO - Started process (PID=39372) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:52:38.778-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:52:38.780-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:52:38.779-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:52:38.796-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:52:38.810-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:52:38.810-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:52:38.823-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:52:38.823-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:52:38.825-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:52:38.824-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:52:38.825-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:52:38.825-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:52:38.827-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:52:38.827-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:52:38.846-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.113 seconds
[2024-09-08T15:53:51.202-0300] {processor.py:186} INFO - Started process (PID=40874) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:53:51.204-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:53:51.205-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:53:51.205-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:53:51.221-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:53:51.236-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:53:51.236-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:53:51.252-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:53:51.252-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:53:51.253-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:53:51.253-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:53:51.254-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:53:51.254-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:53:51.255-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:53:51.255-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:53:51.278-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.081 seconds
[2024-09-08T15:55:06.675-0300] {processor.py:186} INFO - Started process (PID=41166) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:55:06.732-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:55:06.733-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:55:06.733-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:55:06.749-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:55:06.763-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:55:06.763-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:55:06.778-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:55:06.778-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:55:06.780-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:55:06.779-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:55:06.780-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:55:06.780-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:55:06.781-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:55:06.781-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:55:06.800-0300] {processor.py:208} INFO - Processing /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py took 0.128 seconds
[2024-09-08T15:56:07.560-0300] {processor.py:186} INFO - Started process (PID=41636) to work on /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:56:07.561-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:56:07.562-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:07.562-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:56:07.577-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:56:07.592-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:07.591-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:56:07.599-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:07.599-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T15:56:07.600-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:07.600-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T15:56:07.600-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:07.600-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T15:56:07.600-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:07.600-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T15:56:07.606-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:07.606-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:56:07.607-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:07.607-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:56:07.607-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:07.607-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:56:07.607-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:07.607-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:56:08.033-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:08.032-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T15:56:08.034-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:08.034-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T15:56:08.035-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:08.035-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T15:56:08.036-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:08.035-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T15:56:08.036-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:56:08.036-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:56:08.037-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/share/pipx/venvs/apache-airflow/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T15:58:17.197-0300] {processor.py:186} INFO - Started process (PID=42991) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:58:17.198-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T15:58:17.199-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.199-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:58:17.213-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T15:58:17.316-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.316-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:58:17.323-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.323-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T15:58:17.324-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.324-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T15:58:17.324-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.324-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T15:58:17.325-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.324-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T15:58:17.330-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.329-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:58:17.330-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.330-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:58:17.330-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.330-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T15:58:17.331-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.331-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T15:58:17.575-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.573-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T15:58:17.576-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.576-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T15:58:17.578-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.577-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T15:58:17.579-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.579-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T15:58:17.580-0300] {logging_mixin.py:190} INFO - [2024-09-08T15:58:17.580-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T15:58:17.581-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:01:52.551-0300] {processor.py:186} INFO - Started process (PID=45223) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:01:52.553-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:01:52.556-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:52.555-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:01:52.588-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:01:52.745-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:52.745-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:01:52.754-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:52.754-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:01:52.755-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:52.755-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:01:52.756-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:52.755-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:01:52.756-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:52.756-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:01:52.763-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:52.763-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:01:52.764-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:52.763-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:01:52.764-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:52.764-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:01:52.764-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:52.764-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:01:53.113-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:53.112-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:01:53.114-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:53.114-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:01:53.115-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:53.115-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:01:53.116-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:53.116-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:01:53.117-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:01:53.117-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:01:53.117-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:05:01.771-0300] {processor.py:186} INFO - Started process (PID=47022) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:05:01.773-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:05:01.775-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:01.774-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:05:01.790-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:05:01.876-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:01.876-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:05:01.886-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:01.886-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:05:01.887-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:01.887-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:05:01.887-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:01.887-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:05:01.888-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:01.887-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:05:01.893-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:01.893-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:05:01.894-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:01.894-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:05:01.894-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:01.894-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:05:01.894-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:01.894-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:05:02.282-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:02.281-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:05:02.283-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:02.283-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:05:02.284-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:02.284-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:05:02.285-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:02.285-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:05:02.286-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:05:02.286-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:05:02.286-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:08:06.046-0300] {processor.py:186} INFO - Started process (PID=48600) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:08:06.046-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:08:06.048-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.047-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:08:06.063-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:08:06.220-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.220-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:08:06.228-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.227-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:08:06.228-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.228-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:08:06.228-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.228-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:08:06.229-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.229-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:08:06.234-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.233-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:08:06.234-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.234-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:08:06.234-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.234-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:08:06.235-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.235-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:08:06.339-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.338-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:08:06.340-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.340-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:08:06.341-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.341-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:08:06.342-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.341-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:08:06.342-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:08:06.342-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:08:06.343-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:11:15.959-0300] {processor.py:186} INFO - Started process (PID=50508) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:11:15.960-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:11:15.961-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:15.961-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:11:15.975-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:11:16.160-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.160-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:11:16.171-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.171-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:11:16.172-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.171-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:11:16.172-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.172-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:11:16.172-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.172-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:11:16.179-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.179-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:11:16.180-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.180-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:11:16.181-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.181-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:11:16.182-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.182-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:11:16.217-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.214-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:11:16.219-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.219-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:11:16.221-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.220-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:11:16.224-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.223-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:11:16.226-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:11:16.225-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:11:16.227-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:14:00.547-0300] {processor.py:186} INFO - Started process (PID=51989) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:14:00.548-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:14:00.550-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.549-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:14:00.563-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:14:00.632-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.632-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:14:00.639-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.639-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:14:00.640-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.640-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:14:00.640-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.640-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:14:00.640-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.640-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:14:00.646-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.645-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:14:00.646-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.646-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:14:00.646-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.646-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:14:00.647-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.647-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:14:00.703-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.701-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:14:00.704-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.703-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:14:00.705-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.704-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:14:00.706-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.705-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:14:00.706-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:14:00.706-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:14:00.707-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:15:20.659-0300] {processor.py:186} INFO - Started process (PID=52667) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:15:20.659-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:15:20.661-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:20.661-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:15:20.678-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:15:20.807-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:20.807-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:15:20.817-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:20.817-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:15:20.818-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:20.818-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:15:20.818-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:20.818-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:15:20.819-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:20.818-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:15:20.824-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:20.824-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:15:20.825-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:20.825-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:15:20.826-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:20.825-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:15:20.826-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:20.826-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:15:21.173-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:21.171-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:15:21.174-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:21.173-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:15:21.175-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:21.175-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:15:21.178-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:21.177-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:15:21.179-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:15:21.179-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:15:21.180-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:16:38.424-0300] {processor.py:186} INFO - Started process (PID=53242) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:16:38.425-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:16:38.427-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.426-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:16:38.441-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:16:38.511-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.510-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:16:38.521-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.521-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:16:38.522-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.522-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:16:38.522-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.522-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:16:38.523-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.523-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:16:38.529-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.528-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:16:38.529-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.529-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:16:38.529-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.529-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:16:38.530-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.530-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:16:38.913-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.910-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:16:38.914-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.913-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:16:38.915-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.914-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:16:38.916-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.915-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:16:38.916-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:16:38.916-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:16:38.917-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:17:50.976-0300] {processor.py:186} INFO - Started process (PID=53844) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:17:50.977-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:17:50.979-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:50.978-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:17:50.995-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:17:51.084-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.084-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:17:51.092-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.092-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:17:51.093-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.093-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:17:51.093-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.093-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:17:51.094-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.094-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:17:51.101-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.101-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:17:51.102-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.102-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:17:51.102-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.102-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:17:51.103-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.102-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:17:51.494-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.493-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:17:51.496-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.495-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:17:51.496-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.496-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:17:51.497-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.497-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:17:51.498-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:17:51.498-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:17:51.499-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:19:10.992-0300] {processor.py:186} INFO - Started process (PID=54542) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:19:10.993-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:19:10.994-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:10.994-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:19:11.009-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:19:11.083-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.083-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:19:11.091-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.090-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:19:11.091-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.091-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:19:11.091-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.091-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:19:11.092-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.092-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:19:11.097-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.097-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:19:11.097-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.097-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:19:11.098-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.097-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:19:11.098-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.098-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:19:11.360-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.359-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:19:11.361-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.361-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:19:11.362-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.362-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:19:11.363-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.363-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:19:11.363-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:19:11.363-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:19:11.364-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:20:33.869-0300] {processor.py:186} INFO - Started process (PID=55464) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:20:33.870-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:20:33.871-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:33.871-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:20:33.889-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:20:33.968-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:33.968-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:20:33.978-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:33.978-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:20:33.979-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:33.978-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:20:33.979-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:33.979-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:20:33.980-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:33.979-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:20:33.987-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:33.987-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:20:33.988-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:33.988-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:20:33.989-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:33.988-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:20:33.989-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:33.989-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:20:34.367-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:34.365-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:20:34.369-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:34.368-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:20:34.370-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:34.369-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:20:34.371-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:34.371-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:20:34.372-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:20:34.372-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:20:34.373-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:21:52.333-0300] {processor.py:186} INFO - Started process (PID=57358) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:21:52.333-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:21:52.334-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.334-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:21:52.347-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:21:52.417-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.417-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:21:52.424-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.424-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:21:52.425-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.425-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:21:52.425-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.425-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:21:52.425-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.425-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:21:52.431-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.431-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:21:52.431-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.431-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:21:52.432-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.432-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:21:52.432-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.432-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:21:52.544-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.543-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:21:52.545-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.545-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:21:52.546-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.546-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:21:52.547-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.547-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:21:52.547-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:21:52.547-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:21:52.548-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:23:26.761-0300] {processor.py:186} INFO - Started process (PID=58326) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:23:26.762-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:23:26.762-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:26.762-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:23:26.777-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:23:26.853-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:26.853-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:23:26.861-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:26.861-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:23:26.861-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:26.861-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:23:26.862-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:26.862-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:23:26.862-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:26.862-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:23:26.868-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:26.868-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:23:26.869-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:26.869-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:23:26.869-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:26.869-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:23:26.869-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:26.869-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:23:27.235-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:27.233-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:23:27.236-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:27.235-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:23:27.237-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:27.236-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:23:27.238-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:27.237-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:23:27.238-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:23:27.238-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:23:27.239-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:24:45.391-0300] {processor.py:186} INFO - Started process (PID=59062) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:24:45.391-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:24:45.392-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.392-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:24:45.406-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:24:45.478-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.478-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:24:45.486-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.486-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:24:45.487-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.487-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:24:45.487-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.487-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:24:45.487-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.487-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:24:45.493-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.493-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:24:45.493-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.493-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:24:45.494-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.494-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:24:45.494-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.494-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:24:45.911-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.910-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:24:45.912-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.912-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:24:45.913-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.913-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:24:45.914-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.914-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:24:45.915-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:24:45.915-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:24:45.915-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:32:16.580-0300] {processor.py:186} INFO - Started process (PID=65949) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:32:16.581-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:32:16.583-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.583-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:32:16.597-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:32:16.676-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.675-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:32:16.683-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.683-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:32:16.683-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.683-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:32:16.684-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.684-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:32:16.684-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.684-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:32:16.690-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.690-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:32:16.691-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.691-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:32:16.691-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.691-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:32:16.691-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.691-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:32:16.775-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.773-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:32:16.776-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.775-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:32:16.777-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.776-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:32:16.778-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.777-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:32:16.778-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:32:16.778-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:32:16.779-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:34:06.577-0300] {processor.py:186} INFO - Started process (PID=67521) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:34:06.578-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:34:06.580-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:06.580-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:34:06.594-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:34:06.668-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:06.668-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:34:06.675-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:06.675-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:34:06.676-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:06.676-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:34:06.676-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:06.676-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:34:06.677-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:06.676-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:34:06.683-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:06.683-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:34:06.684-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:06.684-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:34:06.684-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:06.684-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:34:06.684-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:06.684-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:34:07.124-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:07.122-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:34:07.125-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:07.124-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:34:07.126-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:07.125-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:34:07.127-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:07.126-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:34:07.127-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:34:07.127-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:34:07.128-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:37:46.987-0300] {processor.py:186} INFO - Started process (PID=70564) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:37:46.988-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:37:46.989-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:46.989-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:37:47.002-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:37:47.072-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.072-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:37:47.080-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.079-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:37:47.080-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.080-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:37:47.080-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.080-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:37:47.081-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.081-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:37:47.086-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.086-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:37:47.086-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.086-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:37:47.087-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.086-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:37:47.087-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.087-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:37:47.411-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.409-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:37:47.412-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.411-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:37:47.412-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.412-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:37:47.413-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.413-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:37:47.414-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:37:47.414-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:37:47.415-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:41:19.647-0300] {processor.py:186} INFO - Started process (PID=73487) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:41:19.648-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:41:19.649-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:19.649-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:41:19.663-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:41:19.734-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:19.734-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:41:19.745-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:19.745-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:41:19.746-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:19.746-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:41:19.747-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:19.746-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:41:19.747-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:19.747-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:41:19.753-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:19.753-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:41:19.753-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:19.753-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:41:19.754-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:19.754-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:41:19.754-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:19.754-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:41:20.094-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:20.093-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:41:20.095-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:20.095-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:41:20.096-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:20.096-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:41:20.097-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:20.097-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:41:20.098-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:41:20.097-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:41:20.098-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:44:52.496-0300] {processor.py:186} INFO - Started process (PID=76454) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:44:52.496-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:44:52.498-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.497-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:44:52.511-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:44:52.580-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.580-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:44:52.588-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.587-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:44:52.588-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.588-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:44:52.588-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.588-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:44:52.589-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.589-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:44:52.595-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.595-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:44:52.596-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.595-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:44:52.596-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.596-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:44:52.596-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.596-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:44:52.899-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.897-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:44:52.900-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.899-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:44:52.901-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.900-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:44:52.902-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.901-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:44:52.902-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:44:52.902-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:44:52.903-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:48:24.784-0300] {processor.py:186} INFO - Started process (PID=79349) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:48:24.788-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:48:24.789-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:24.789-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:48:24.802-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:48:24.870-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:24.870-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:48:24.878-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:24.878-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:48:24.878-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:24.878-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:48:24.879-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:24.879-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:48:24.879-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:24.879-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:48:24.884-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:24.884-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:48:24.885-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:24.885-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:48:24.885-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:24.885-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:48:24.885-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:24.885-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:48:25.371-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:25.370-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:48:25.372-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:25.372-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:48:25.373-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:25.373-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:48:25.374-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:25.374-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:48:25.375-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:48:25.374-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:48:25.375-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:51:58.169-0300] {processor.py:186} INFO - Started process (PID=82324) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:51:58.169-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:51:58.170-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.170-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:51:58.184-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:51:58.254-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.254-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:51:58.263-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.262-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:51:58.263-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.263-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:51:58.263-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.263-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:51:58.264-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.264-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:51:58.269-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.269-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:51:58.270-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.269-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:51:58.270-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.270-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:51:58.270-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.270-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:51:58.607-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.606-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:51:58.608-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.608-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:51:58.609-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.608-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:51:58.610-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.609-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:51:58.610-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:51:58.610-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:51:58.611-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:55:30.063-0300] {processor.py:186} INFO - Started process (PID=85371) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:55:30.064-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:55:30.065-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.065-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:55:30.078-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:55:30.146-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.146-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:55:30.154-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.154-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:55:30.155-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.154-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:55:30.155-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.155-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:55:30.155-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.155-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:55:30.160-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.160-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:55:30.161-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.161-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:55:30.161-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.161-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:55:30.162-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.161-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:55:30.548-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.546-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:55:30.549-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.548-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:55:30.550-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.549-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:55:30.551-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.550-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:55:30.551-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:55:30.551-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:55:30.552-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:57:43.869-0300] {processor.py:186} INFO - Started process (PID=87198) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:57:43.870-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T16:57:43.871-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:43.871-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:57:43.884-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T16:57:43.957-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:43.956-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:57:43.964-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:43.963-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T16:57:43.964-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:43.964-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T16:57:43.964-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:43.964-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T16:57:43.965-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:43.965-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T16:57:43.971-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:43.971-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:57:43.972-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:43.971-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:57:43.972-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:43.972-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T16:57:43.972-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:43.972-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T16:57:44.221-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:44.220-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:57:44.222-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:44.222-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:57:44.223-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:44.222-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:57:44.224-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:44.223-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T16:57:44.224-0300] {logging_mixin.py:190} INFO - [2024-09-08T16:57:44.224-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T16:57:44.225-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:01:08.553-0300] {processor.py:186} INFO - Started process (PID=89869) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:01:08.553-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:01:08.555-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:08.555-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:01:08.570-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:01:08.639-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:08.639-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:01:08.647-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:08.647-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:01:08.647-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:08.647-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:01:08.648-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:08.648-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:01:08.648-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:08.648-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:01:08.653-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:08.653-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:01:08.654-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:08.654-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:01:08.654-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:08.654-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:01:08.654-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:08.654-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:01:09.129-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:09.128-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:01:09.130-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:09.130-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:01:09.131-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:09.131-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:01:09.132-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:09.131-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:01:09.132-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:01:09.132-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:01:09.133-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:04:35.535-0300] {processor.py:186} INFO - Started process (PID=92573) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:04:35.536-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:04:35.537-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.537-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:04:35.550-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:04:35.620-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.620-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:04:35.627-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.627-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:04:35.627-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.627-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:04:35.628-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.628-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:04:35.628-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.628-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:04:35.633-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.633-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:04:35.634-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.634-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:04:35.634-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.634-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:04:35.634-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.634-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:04:35.853-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.851-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:04:35.854-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.853-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:04:35.854-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.854-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:04:35.855-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.855-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:04:35.856-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:04:35.856-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:04:35.856-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:08:01.719-0300] {processor.py:186} INFO - Started process (PID=95265) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:08:01.720-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:08:01.721-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.721-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:08:01.735-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:08:01.805-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.805-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:08:01.812-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.812-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:08:01.812-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.812-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:08:01.813-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.813-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:08:01.813-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.813-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:08:01.818-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.818-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:08:01.819-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.818-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:08:01.819-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.819-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:08:01.819-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.819-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:08:01.878-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.877-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:08:01.879-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.879-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:08:01.880-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.880-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:08:01.881-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.881-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:08:01.881-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:08:01.881-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:08:01.882-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:11:31.990-0300] {processor.py:186} INFO - Started process (PID=98023) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:11:31.993-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:11:31.994-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:31.994-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:11:32.009-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:11:32.077-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.077-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:11:32.084-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.084-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:11:32.084-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.084-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:11:32.085-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.084-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:11:32.085-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.085-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:11:32.090-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.090-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:11:32.091-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.090-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:11:32.091-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.091-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:11:32.091-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.091-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:11:32.238-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.236-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:11:32.239-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.238-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:11:32.240-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.239-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:11:32.241-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.240-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:11:32.241-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:11:32.241-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:11:32.242-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:15:02.754-0300] {processor.py:186} INFO - Started process (PID=100803) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:15:02.755-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:15:02.757-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:02.756-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:15:02.771-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:15:02.841-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:02.841-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:15:02.847-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:02.847-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:15:02.848-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:02.848-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:15:02.848-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:02.848-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:15:02.849-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:02.849-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:15:02.854-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:02.854-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:15:02.854-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:02.854-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:15:02.855-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:02.854-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:15:02.855-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:02.855-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:15:03.107-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:03.105-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:15:03.108-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:03.107-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:15:03.109-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:03.108-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:15:03.109-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:03.109-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:15:03.110-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:15:03.110-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:15:03.111-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:18:33.338-0300] {processor.py:186} INFO - Started process (PID=103589) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:18:33.339-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:18:33.340-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.340-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:18:33.353-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:18:33.423-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.423-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:18:33.430-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.430-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:18:33.431-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.430-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:18:33.431-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.431-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:18:33.431-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.431-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:18:33.438-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.437-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:18:33.438-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.438-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:18:33.438-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.438-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:18:33.439-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.439-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:18:33.771-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.769-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:18:33.772-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.771-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:18:33.773-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.772-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:18:33.774-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.773-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:18:33.774-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:18:33.774-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:18:33.775-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:22:04.615-0300] {processor.py:186} INFO - Started process (PID=106409) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:22:04.616-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:22:04.617-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:04.617-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:22:04.632-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:22:04.699-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:04.699-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:22:04.707-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:04.707-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:22:04.707-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:04.707-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:22:04.708-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:04.708-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:22:04.708-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:04.708-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:22:04.713-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:04.713-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:22:04.714-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:04.714-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:22:04.714-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:04.714-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:22:04.714-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:04.714-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:22:05.221-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:05.219-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:22:05.222-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:05.221-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:22:05.223-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:05.222-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:22:05.224-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:05.223-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:22:05.224-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:22:05.224-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:22:05.225-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:25:31.245-0300] {processor.py:186} INFO - Started process (PID=109155) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:25:31.246-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:25:31.247-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.247-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:25:31.260-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:25:31.328-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.328-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:25:31.335-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.335-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:25:31.336-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.335-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:25:31.336-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.336-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:25:31.336-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.336-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:25:31.341-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.341-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:25:31.342-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.342-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:25:31.342-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.342-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:25:31.343-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.343-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:25:31.596-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.595-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:25:31.597-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.597-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:25:31.598-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.598-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:25:31.599-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.599-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:25:31.599-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:25:31.599-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:25:31.600-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:28:52.380-0300] {processor.py:186} INFO - Started process (PID=111851) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:28:52.381-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:28:52.382-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.382-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:28:52.397-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:28:52.470-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.470-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:28:52.477-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.477-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:28:52.478-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.478-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:28:52.478-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.478-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:28:52.478-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.478-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:28:52.483-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.483-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:28:52.484-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.484-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:28:52.484-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.484-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:28:52.485-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.484-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:28:52.679-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.678-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:28:52.680-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.680-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:28:52.681-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.681-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:28:52.682-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.682-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:28:52.682-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:28:52.682-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:28:52.683-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:32:07.758-0300] {processor.py:186} INFO - Started process (PID=114475) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:32:07.759-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:32:07.760-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:07.760-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:32:07.774-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:32:07.844-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:07.844-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:32:07.851-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:07.851-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:32:07.852-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:07.851-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:32:07.852-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:07.852-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:32:07.852-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:07.852-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:32:07.858-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:07.858-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:32:07.858-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:07.858-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:32:07.859-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:07.859-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:32:07.859-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:07.859-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:32:08.346-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:08.344-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:32:08.347-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:08.346-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:32:08.348-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:08.347-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:32:08.349-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:08.348-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:32:08.349-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:32:08.349-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:32:08.350-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:35:30.371-0300] {processor.py:186} INFO - Started process (PID=117121) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:35:30.372-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:35:30.373-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.373-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:35:30.386-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:35:30.459-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.458-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:35:30.465-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.465-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:35:30.466-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.466-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:35:30.466-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.466-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:35:30.467-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.466-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:35:30.472-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.472-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:35:30.472-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.472-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:35:30.473-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.473-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:35:30.473-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.473-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:35:30.541-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.540-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:35:30.542-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.542-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:35:30.543-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.543-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:35:30.544-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.544-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:35:30.545-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:35:30.544-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:35:30.545-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:38:36.844-0300] {processor.py:186} INFO - Started process (PID=119552) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:38:36.845-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:38:36.846-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:36.846-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:38:36.860-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:38:36.930-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:36.930-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:38:36.937-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:36.936-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:38:36.937-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:36.937-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:38:36.937-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:36.937-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:38:36.938-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:36.938-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:38:36.943-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:36.943-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:38:36.943-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:36.943-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:38:36.944-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:36.943-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:38:36.944-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:36.944-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:38:37.101-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:37.100-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:38:37.102-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:37.102-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:38:37.103-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:37.103-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:38:37.104-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:37.104-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:38:37.105-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:38:37.104-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:38:37.105-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:41:45.148-0300] {processor.py:186} INFO - Started process (PID=122083) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:41:45.254-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:41:45.255-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.255-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:41:45.269-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:41:45.339-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.338-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:41:45.345-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.345-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:41:45.346-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.346-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:41:45.346-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.346-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:41:45.346-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.346-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:41:45.352-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.352-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:41:45.352-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.352-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:41:45.352-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.352-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:41:45.353-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.353-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:41:45.561-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.560-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:41:45.562-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.562-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:41:45.563-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.563-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:41:45.564-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.564-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:41:45.565-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:41:45.565-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:41:45.565-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:44:56.605-0300] {processor.py:186} INFO - Started process (PID=124615) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:44:56.606-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:44:56.607-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:56.607-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:44:56.622-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:44:56.691-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:56.691-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:44:56.698-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:56.698-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:44:56.698-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:56.698-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:44:56.698-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:56.698-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:44:56.699-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:56.699-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:44:56.704-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:56.704-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:44:56.704-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:56.704-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:44:56.705-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:56.705-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:44:56.705-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:56.705-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:44:57.209-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:57.208-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:44:57.210-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:57.210-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:44:57.211-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:57.211-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:44:57.212-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:57.212-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:44:57.213-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:44:57.213-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:44:57.213-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:49:08.109-0300] {processor.py:186} INFO - Started process (PID=128580) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:49:08.110-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:49:08.111-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.111-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:49:08.126-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:49:08.204-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.204-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:49:08.212-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.212-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:49:08.212-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.212-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:49:08.213-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.212-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:49:08.213-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.213-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:49:08.220-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.220-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:49:08.221-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.221-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:49:08.221-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.221-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:49:08.221-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.221-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:49:08.589-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.588-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:49:08.590-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.590-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:49:08.591-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.591-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:49:08.592-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.591-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:49:08.592-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:49:08.592-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:49:08.593-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:53:30.879-0300] {processor.py:186} INFO - Started process (PID=132170) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:53:30.881-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:53:30.883-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:30.883-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:53:30.897-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:53:30.972-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:30.972-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:53:30.980-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:30.980-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:53:30.980-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:30.980-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:53:30.981-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:30.981-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:53:30.981-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:30.981-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:53:30.986-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:30.986-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:53:30.987-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:30.987-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:53:30.987-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:30.987-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:53:30.988-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:30.988-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:53:31.467-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:31.464-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:53:31.468-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:31.467-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:53:31.469-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:31.468-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:53:31.470-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:31.469-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:53:31.470-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:53:31.470-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:53:31.471-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:57:17.713-0300] {processor.py:186} INFO - Started process (PID=135127) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:57:17.714-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T17:57:17.716-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.715-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:57:17.730-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T17:57:17.803-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.803-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:57:17.811-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.811-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T17:57:17.812-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.812-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T17:57:17.812-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.812-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T17:57:17.813-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.812-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T17:57:17.819-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.818-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:57:17.819-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.819-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:57:17.820-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.819-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T17:57:17.820-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.820-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T17:57:17.994-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.992-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:57:17.996-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.995-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:57:17.997-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.996-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:57:17.997-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.997-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T17:57:17.998-0300] {logging_mixin.py:190} INFO - [2024-09-08T17:57:17.998-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T17:57:17.999-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:01:08.661-0300] {processor.py:186} INFO - Started process (PID=138199) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:01:08.662-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:01:08.664-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.663-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:01:08.679-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:01:08.755-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.754-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:01:08.762-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.762-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:01:08.762-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.762-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:01:08.762-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.762-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:01:08.763-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.763-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:01:08.768-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.768-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:01:08.769-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.769-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:01:08.769-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.769-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:01:08.770-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.770-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:01:08.837-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.835-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:01:08.839-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.838-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:01:08.839-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.839-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:01:08.840-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.840-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:01:08.841-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:01:08.841-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:01:08.841-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:04:50.506-0300] {processor.py:186} INFO - Started process (PID=141115) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:04:50.510-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:04:50.512-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.511-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:04:50.525-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:04:50.596-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.596-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:04:50.603-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.603-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:04:50.603-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.603-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:04:50.604-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.604-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:04:50.604-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.604-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:04:50.609-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.609-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:04:50.610-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.610-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:04:50.610-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.610-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:04:50.610-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.610-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:04:50.828-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.826-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:04:50.829-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.828-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:04:50.830-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.830-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:04:50.831-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.831-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:04:50.832-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:04:50.832-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:04:50.833-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:09:13.527-0300] {processor.py:186} INFO - Started process (PID=145481) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:09:13.528-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:09:13.529-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:13.529-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:09:13.544-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:09:13.623-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:13.623-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:09:13.630-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:13.630-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:09:13.630-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:13.630-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:09:13.631-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:13.631-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:09:13.631-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:13.631-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:09:13.638-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:13.638-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:09:13.639-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:13.639-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:09:13.639-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:13.639-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:09:13.640-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:13.639-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:09:14.081-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:14.079-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:09:14.083-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:14.082-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:09:14.084-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:14.084-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:09:14.085-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:14.085-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:09:14.086-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:09:14.086-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:09:14.087-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:14:10.699-0300] {processor.py:186} INFO - Started process (PID=149866) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:14:10.710-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:14:10.711-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:10.711-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:14:10.726-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:14:10.796-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:10.796-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:14:10.804-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:10.804-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:14:10.805-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:10.804-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:14:10.805-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:10.805-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:14:10.805-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:10.805-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:14:10.899-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:10.899-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:14:10.900-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:10.899-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:14:10.900-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:10.900-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:14:10.900-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:10.900-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:14:11.291-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:11.290-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:14:11.292-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:11.292-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:14:11.293-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:11.293-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:14:11.294-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:11.294-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:14:11.295-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:14:11.295-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:14:11.295-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:17:30.099-0300] {processor.py:186} INFO - Started process (PID=152073) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:17:30.106-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:17:30.107-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.107-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:17:30.121-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:17:30.199-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.199-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:17:30.298-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.298-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:17:30.298-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.298-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:17:30.299-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.299-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:17:30.299-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.299-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:17:30.304-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.304-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:17:30.304-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.304-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:17:30.305-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.305-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:17:30.305-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.305-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:17:30.455-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.454-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:17:30.457-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.456-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:17:30.458-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.457-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:17:30.459-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.458-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:17:30.459-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:17:30.459-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:17:30.460-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:18:57.250-0300] {processor.py:186} INFO - Started process (PID=152725) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:18:57.250-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:18:57.251-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.251-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:18:57.265-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:18:57.336-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.336-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:18:57.431-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.431-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:18:57.431-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.431-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:18:57.432-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.431-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:18:57.432-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.432-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:18:57.437-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.437-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:18:57.437-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.437-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:18:57.438-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.437-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:18:57.438-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.438-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:18:57.485-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.484-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:18:57.486-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.486-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:18:57.487-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.487-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:18:57.488-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.488-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:18:57.489-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:18:57.489-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:18:57.490-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:20:25.108-0300] {processor.py:186} INFO - Started process (PID=153358) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:20:25.109-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:20:25.110-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.110-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:20:25.124-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:20:25.291-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.291-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:20:25.300-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.300-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:20:25.301-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.301-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:20:25.301-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.301-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:20:25.302-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.302-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:20:25.308-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.308-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:20:25.309-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.309-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:20:25.309-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.309-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:20:25.310-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.309-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:20:25.734-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.733-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:20:25.735-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.734-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:20:25.736-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.735-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:20:25.737-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.736-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:20:25.737-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:20:25.737-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:20:25.738-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:22:33.953-0300] {processor.py:186} INFO - Started process (PID=154851) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:22:33.954-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:22:33.955-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:33.955-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:22:33.969-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:22:34.135-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.135-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:22:34.142-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.142-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:22:34.143-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.143-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:22:34.143-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.143-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:22:34.144-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.144-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:22:34.152-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.152-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:22:34.152-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.152-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:22:34.153-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.153-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:22:34.153-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.153-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:22:34.386-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.385-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:22:34.387-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.387-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:22:34.388-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.388-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:22:34.389-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.389-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:22:34.390-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:22:34.390-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:22:34.391-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:24:15.262-0300] {processor.py:186} INFO - Started process (PID=156136) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:24:15.262-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:24:15.263-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.263-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:24:15.278-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:24:15.451-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.451-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:24:15.459-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.459-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:24:15.459-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.459-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:24:15.460-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.460-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:24:15.460-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.460-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:24:15.466-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.466-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:24:15.466-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.466-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:24:15.467-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.466-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:24:15.467-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.467-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:24:15.549-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.547-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:24:15.550-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.549-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:24:15.551-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.550-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:24:15.551-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.551-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:24:15.552-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:24:15.552-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:24:15.554-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:25:51.846-0300] {processor.py:186} INFO - Started process (PID=156865) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:25:51.847-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:25:51.848-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:51.847-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:25:51.867-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:25:52.098-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.098-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:25:52.108-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.107-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:25:52.108-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.108-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:25:52.109-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.108-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:25:52.109-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.109-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:25:52.115-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.115-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:25:52.116-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.116-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:25:52.117-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.116-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:25:52.117-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.117-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:25:52.329-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.327-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:25:52.330-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.330-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:25:52.332-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.331-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:25:52.333-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.332-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:25:52.333-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:25:52.333-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:25:52.335-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:28:53.881-0300] {processor.py:186} INFO - Started process (PID=158959) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:28:53.883-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:28:53.885-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:53.884-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:28:54.061-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:28:54.152-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.152-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:28:54.161-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.161-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:28:54.162-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.162-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:28:54.162-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.162-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:28:54.163-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.163-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:28:54.170-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.170-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:28:54.171-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.170-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:28:54.171-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.171-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:28:54.172-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.171-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:28:54.390-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.388-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:28:54.393-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.392-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:28:54.394-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.393-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:28:54.395-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.394-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:28:54.396-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:28:54.396-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:28:54.398-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:30:25.513-0300] {processor.py:186} INFO - Started process (PID=159919) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:30:25.515-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:30:25.515-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:25.515-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:30:25.530-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:30:25.604-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:25.603-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:30:25.611-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:25.611-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:30:25.612-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:25.612-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:30:25.612-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:25.612-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:30:25.612-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:25.612-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:30:25.619-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:25.619-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:30:25.619-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:25.619-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:30:25.620-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:25.620-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:30:25.620-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:25.620-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:30:26.005-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:26.004-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:30:26.006-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:26.006-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:30:26.007-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:26.006-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:30:26.008-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:26.007-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:30:26.008-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:30:26.008-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:30:26.009-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:31:54.233-0300] {processor.py:186} INFO - Started process (PID=160636) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:31:54.234-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:31:54.235-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.235-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:31:54.250-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:31:54.344-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.343-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:31:54.351-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.351-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:31:54.351-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.351-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:31:54.352-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.351-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:31:54.352-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.352-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:31:54.361-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.361-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:31:54.362-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.362-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:31:54.362-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.362-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:31:54.363-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.363-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:31:54.559-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.558-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:31:54.560-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.559-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:31:54.561-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.560-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:31:54.562-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.561-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:31:54.563-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:31:54.562-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:31:54.564-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:33:55.675-0300] {processor.py:186} INFO - Started process (PID=161785) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:33:55.675-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:33:55.676-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:55.676-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:33:55.691-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:33:55.769-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:55.769-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:33:55.777-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:55.777-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:33:55.778-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:55.777-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:33:55.778-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:55.778-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:33:55.778-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:55.778-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:33:55.784-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:55.784-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:33:55.785-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:55.785-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:33:55.785-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:55.785-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:33:55.786-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:55.786-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:33:56.025-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:56.023-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:33:56.026-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:56.025-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:33:56.027-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:56.026-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:33:56.028-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:56.027-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:33:56.028-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:33:56.028-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:33:56.029-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:35:13.546-0300] {processor.py:186} INFO - Started process (PID=162368) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:35:13.547-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:35:13.548-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:13.547-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:35:13.561-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:35:13.640-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:13.640-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:35:13.649-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:13.649-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:35:13.649-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:13.649-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:35:13.650-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:13.650-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:35:13.650-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:13.650-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:35:13.655-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:13.655-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:35:13.656-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:13.656-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:35:13.656-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:13.656-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:35:13.657-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:13.656-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:35:14.138-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:14.136-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:35:14.140-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:14.140-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:35:14.142-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:14.141-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:35:14.144-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:14.143-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:35:14.144-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:35:14.144-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:35:14.145-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:36:30.930-0300] {processor.py:186} INFO - Started process (PID=162929) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:36:30.931-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:36:30.932-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:30.932-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:36:30.952-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:36:31.031-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.030-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:36:31.040-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.040-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:36:31.040-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.040-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:36:31.041-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.040-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:36:31.041-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.041-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:36:31.046-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.046-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:36:31.047-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.047-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:36:31.047-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.047-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:36:31.048-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.047-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:36:31.542-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.540-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:36:31.543-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.542-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:36:31.544-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.544-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:36:31.545-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.545-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:36:31.546-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:36:31.546-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:36:31.546-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:37:50.783-0300] {processor.py:186} INFO - Started process (PID=163510) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:37:50.784-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:37:50.785-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:50.784-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:37:50.798-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:37:50.876-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:50.876-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:37:50.885-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:50.885-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:37:50.885-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:50.885-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:37:50.886-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:50.885-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:37:50.886-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:50.886-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:37:50.891-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:50.891-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:37:50.892-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:50.892-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:37:50.892-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:50.892-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:37:50.893-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:50.892-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:37:51.214-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:51.212-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:37:51.215-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:51.214-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:37:51.216-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:51.215-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:37:51.217-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:51.216-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:37:51.217-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:37:51.217-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:37:51.218-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:39:13.771-0300] {processor.py:186} INFO - Started process (PID=164239) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:39:13.771-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:39:13.772-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:13.772-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:39:13.787-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:39:13.868-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:13.868-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:39:13.876-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:13.876-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:39:13.877-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:13.877-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:39:13.877-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:13.877-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:39:13.878-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:13.877-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:39:13.886-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:13.885-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:39:13.886-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:13.886-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:39:13.887-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:13.887-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:39:13.887-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:13.887-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:39:14.262-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:14.261-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:39:14.263-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:14.263-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:39:14.264-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:14.264-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:39:14.265-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:14.265-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:39:14.266-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:39:14.265-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:39:14.266-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:40:35.506-0300] {processor.py:186} INFO - Started process (PID=164809) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:40:35.507-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:40:35.508-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.508-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:40:35.523-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:40:35.599-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.599-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:40:35.607-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.607-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:40:35.607-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.607-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:40:35.608-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.608-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:40:35.608-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.608-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:40:35.614-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.614-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:40:35.615-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.615-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:40:35.615-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.615-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:40:35.616-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.616-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:40:35.965-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.963-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:40:35.966-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.966-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:40:35.967-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.967-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:40:35.968-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.968-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:40:35.968-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:40:35.968-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:40:35.969-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:41:54.120-0300] {processor.py:186} INFO - Started process (PID=165899) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:41:54.120-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:41:54.121-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.121-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:41:54.136-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:41:54.215-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.215-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:41:54.222-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.222-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:41:54.223-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.223-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:41:54.223-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.223-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:41:54.223-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.223-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:41:54.230-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.229-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:41:54.230-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.230-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:41:54.230-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.230-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:41:54.231-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.231-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:41:54.704-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.702-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:41:54.705-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.704-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:41:54.706-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.705-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:41:54.707-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.706-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:41:54.707-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:41:54.707-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:41:54.708-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:43:12.215-0300] {processor.py:186} INFO - Started process (PID=167170) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:43:12.223-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:43:12.224-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.224-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:43:12.240-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:43:12.318-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.318-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:43:12.326-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.326-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:43:12.327-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.327-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:43:12.327-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.327-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:43:12.327-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.327-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:43:12.333-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.333-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:43:12.333-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.333-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:43:12.334-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.334-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:43:12.334-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.334-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:43:12.659-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.657-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:43:12.660-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.659-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:43:12.661-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.660-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:43:12.661-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.661-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:43:12.662-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:43:12.662-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:43:12.663-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:44:35.194-0300] {processor.py:186} INFO - Started process (PID=167787) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:44:35.195-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:44:35.196-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.196-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:44:35.209-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:44:35.280-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.280-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:44:35.288-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.288-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:44:35.288-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.288-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:44:35.289-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.288-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:44:35.289-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.289-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:44:35.294-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.294-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:44:35.295-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.295-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:44:35.295-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.295-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:44:35.296-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.295-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:44:35.781-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.779-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:44:35.782-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.781-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:44:35.783-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.782-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:44:35.783-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.783-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:44:35.784-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:44:35.784-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:44:35.785-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:45:52.725-0300] {processor.py:186} INFO - Started process (PID=168342) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:45:52.725-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:45:52.726-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.726-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:45:52.740-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:45:52.812-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.812-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:45:52.820-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.820-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:45:52.820-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.820-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:45:52.821-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.821-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:45:52.821-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.821-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:45:52.828-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.828-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:45:52.828-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.828-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:45:52.829-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.829-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:45:52.829-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.829-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:45:52.948-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.946-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:45:52.949-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.948-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:45:52.949-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.949-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:45:52.950-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.950-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:45:52.951-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:45:52.951-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:45:52.952-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:47:07.025-0300] {processor.py:186} INFO - Started process (PID=168879) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:47:07.026-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:47:07.027-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.027-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:47:07.042-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:47:07.114-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.114-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:47:07.121-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.121-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:47:07.122-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.121-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:47:07.122-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.122-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:47:07.122-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.122-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:47:07.128-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.128-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:47:07.129-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.129-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:47:07.129-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.129-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:47:07.129-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.129-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:47:07.516-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.515-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:47:07.518-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.517-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:47:07.519-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.518-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:47:07.519-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.519-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:47:07.520-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:47:07.520-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:47:07.521-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:48:19.450-0300] {processor.py:186} INFO - Started process (PID=169579) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:48:19.451-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:48:19.452-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.452-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:48:19.467-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:48:19.541-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.541-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:48:19.549-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.549-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:48:19.549-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.549-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:48:19.550-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.550-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:48:19.550-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.550-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:48:19.557-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.557-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:48:19.557-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.557-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:48:19.558-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.557-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:48:19.558-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.558-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:48:19.841-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.839-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:48:19.842-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.842-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:48:19.843-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.843-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:48:19.844-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.844-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:48:19.845-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:48:19.845-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:48:19.846-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:49:40.309-0300] {processor.py:186} INFO - Started process (PID=170417) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:49:40.310-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:49:40.311-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.310-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:49:40.326-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:49:40.403-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.403-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:49:40.412-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.411-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:49:40.412-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.412-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:49:40.412-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.412-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:49:40.413-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.413-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:49:40.418-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.418-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:49:40.419-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.419-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:49:40.419-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.419-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:49:40.420-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.420-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:49:40.576-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.574-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:49:40.578-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.577-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:49:40.579-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.578-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:49:40.580-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.579-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:49:40.580-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:49:40.580-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:49:40.581-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:50:54.796-0300] {processor.py:186} INFO - Started process (PID=171234) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:50:54.797-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:50:54.798-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:54.798-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:50:54.813-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:50:54.888-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:54.888-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:50:54.896-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:54.896-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:50:54.896-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:54.896-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:50:54.897-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:54.897-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:50:54.897-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:54.897-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:50:54.902-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:54.902-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:50:54.903-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:54.903-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:50:54.903-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:54.903-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:50:54.903-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:54.903-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:50:55.405-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:55.404-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:50:55.406-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:55.406-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:50:55.408-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:55.407-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:50:55.410-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:55.410-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:50:55.411-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:50:55.410-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:50:55.411-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:52:09.277-0300] {processor.py:186} INFO - Started process (PID=172018) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:52:09.278-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:52:09.279-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.279-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:52:09.294-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:52:09.367-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.367-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:52:09.374-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.374-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:52:09.375-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.375-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:52:09.375-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.375-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:52:09.375-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.375-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:52:09.381-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.381-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:52:09.381-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.381-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:52:09.382-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.382-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:52:09.382-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.382-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:52:09.703-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.701-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:52:09.704-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.703-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:52:09.705-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.704-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:52:09.706-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.705-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:52:09.706-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:52:09.706-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:52:09.707-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:53:35.118-0300] {processor.py:186} INFO - Started process (PID=172722) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:53:35.119-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:53:35.120-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.120-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:53:35.133-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:53:35.206-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.206-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:53:35.215-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.215-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:53:35.216-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.215-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:53:35.216-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.216-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:53:35.216-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.216-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:53:35.223-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.223-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:53:35.224-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.224-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:53:35.224-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.224-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:53:35.224-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.224-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:53:35.341-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.340-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:53:35.342-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.342-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:53:35.343-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.342-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:53:35.344-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.343-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:53:35.344-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:53:35.344-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:53:35.345-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:54:49.422-0300] {processor.py:186} INFO - Started process (PID=173287) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:54:49.423-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:54:49.424-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.424-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:54:49.440-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:54:49.517-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.516-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:54:49.526-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.526-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:54:49.527-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.527-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:54:49.527-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.527-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:54:49.528-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.527-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:54:49.534-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.534-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:54:49.535-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.534-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:54:49.535-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.535-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:54:49.535-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.535-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:54:49.939-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.937-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:54:49.940-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.939-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:54:49.941-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.940-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:54:49.942-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.941-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:54:49.942-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:54:49.942-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:54:49.943-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:56:02.626-0300] {processor.py:186} INFO - Started process (PID=173888) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:56:02.627-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:56:02.628-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:02.627-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:56:02.640-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:56:02.716-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:02.716-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:56:02.725-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:02.725-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:56:02.726-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:02.726-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:56:02.726-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:02.726-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:56:02.726-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:02.726-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:56:02.732-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:02.732-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:56:02.732-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:02.732-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:56:02.733-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:02.733-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:56:02.733-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:02.733-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:56:03.158-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:03.157-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:56:03.159-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:03.159-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:56:03.160-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:03.160-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:56:03.161-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:03.161-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:56:03.162-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:56:03.162-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:56:03.163-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:57:22.472-0300] {processor.py:186} INFO - Started process (PID=174530) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:57:22.473-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:57:22.474-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.473-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:57:22.488-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:57:22.560-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.560-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:57:22.567-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.567-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:57:22.568-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.568-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:57:22.568-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.568-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:57:22.568-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.568-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:57:22.574-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.574-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:57:22.575-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.575-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:57:22.575-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.575-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:57:22.576-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.576-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:57:22.739-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.738-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:57:22.740-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.740-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:57:22.741-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.741-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:57:22.742-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.742-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:57:22.742-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:57:22.742-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:57:22.743-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:58:44.274-0300] {processor.py:186} INFO - Started process (PID=175148) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:58:44.275-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T18:58:44.276-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.275-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:58:44.291-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T18:58:44.366-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.366-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:58:44.374-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.374-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T18:58:44.374-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.374-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T18:58:44.375-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.375-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T18:58:44.375-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.375-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T18:58:44.380-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.380-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:58:44.381-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.381-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:58:44.381-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.381-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T18:58:44.382-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.381-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T18:58:44.893-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.891-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:58:44.894-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.893-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:58:44.895-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.894-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:58:44.896-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.895-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T18:58:44.896-0300] {logging_mixin.py:190} INFO - [2024-09-08T18:58:44.896-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T18:58:44.897-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:00:06.727-0300] {processor.py:186} INFO - Started process (PID=176196) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:00:06.728-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T19:00:06.729-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:06.728-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:00:06.743-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:00:06.816-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:06.816-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:00:06.823-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:06.823-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T19:00:06.824-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:06.824-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T19:00:06.824-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:06.824-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T19:00:06.824-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:06.824-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T19:00:06.830-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:06.829-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:00:06.830-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:06.830-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:00:06.830-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:06.830-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:00:06.831-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:06.831-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:00:07.257-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:07.255-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:00:07.258-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:07.258-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:00:07.259-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:07.258-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:00:07.260-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:07.259-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:00:07.260-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:00:07.260-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:00:07.261-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:01:29.845-0300] {processor.py:186} INFO - Started process (PID=176874) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:01:29.846-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T19:01:29.847-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:29.847-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:01:29.863-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:01:29.943-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:29.943-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:01:29.951-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:29.951-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T19:01:29.952-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:29.952-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T19:01:29.952-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:29.952-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T19:01:29.952-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:29.952-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T19:01:29.959-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:29.959-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:01:29.960-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:29.960-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:01:29.960-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:29.960-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:01:29.961-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:29.961-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:01:30.452-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:30.451-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:01:30.453-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:30.453-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:01:30.454-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:30.454-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:01:30.455-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:30.455-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:01:30.456-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:01:30.456-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:01:30.457-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:02:53.496-0300] {processor.py:186} INFO - Started process (PID=177708) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:02:53.496-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T19:02:53.498-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.497-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:02:53.512-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:02:53.591-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.591-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:02:53.599-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.599-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T19:02:53.599-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.599-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T19:02:53.600-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.600-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T19:02:53.600-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.600-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T19:02:53.607-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.607-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:02:53.608-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.608-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:02:53.608-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.608-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:02:53.609-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.609-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:02:53.735-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.734-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:02:53.736-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.736-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:02:53.738-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.737-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:02:53.739-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.738-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:02:53.740-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:02:53.739-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:02:53.740-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:04:15.850-0300] {processor.py:186} INFO - Started process (PID=178354) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:04:15.851-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T19:04:15.852-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:15.851-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:04:15.866-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:04:15.943-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:15.943-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:04:15.952-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:15.952-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T19:04:15.952-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:15.952-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T19:04:15.953-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:15.953-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T19:04:15.953-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:15.953-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T19:04:15.958-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:15.958-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:04:15.959-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:15.959-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:04:15.959-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:15.959-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:04:15.959-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:15.959-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:04:16.394-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:16.392-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:04:16.395-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:16.395-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:04:16.396-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:16.396-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:04:16.397-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:16.397-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:04:16.398-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:04:16.398-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:04:16.399-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:05:39.447-0300] {processor.py:186} INFO - Started process (PID=179015) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:05:39.448-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T19:05:39.448-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.448-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:05:39.463-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:05:39.538-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.538-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:05:39.546-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.546-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T19:05:39.547-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.547-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T19:05:39.547-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.547-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T19:05:39.548-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.548-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T19:05:39.553-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.553-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:05:39.554-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.554-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:05:39.554-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.554-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:05:39.555-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.554-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:05:39.602-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.601-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:05:39.603-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.603-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:05:39.604-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.604-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:05:39.605-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.604-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:05:39.605-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:05:39.605-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:05:39.606-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:06:58.693-0300] {processor.py:186} INFO - Started process (PID=179671) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:06:58.696-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T19:06:58.697-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:58.697-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:06:58.712-0300] {processor.py:925} INFO - DAG(s) 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:06:58.791-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:58.791-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:06:58.799-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:58.799-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T19:06:58.799-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:58.799-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T19:06:58.800-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:58.799-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T19:06:58.800-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:58.800-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T19:06:58.806-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:58.806-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:06:58.806-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:58.806-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:06:58.807-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:58.807-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:06:58.807-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:58.807-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:06:59.148-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:59.146-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:06:59.149-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:59.148-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:06:59.150-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:59.149-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:06:59.151-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:59.150-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:06:59.151-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:06:59.151-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:06:59.152-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:08:18.709-0300] {processor.py:186} INFO - Started process (PID=180686) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:08:18.709-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T19:08:18.710-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:18.710-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:08:18.724-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:08:18.797-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:18.797-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:08:18.807-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:18.807-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T19:08:18.808-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:18.808-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T19:08:18.808-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:18.808-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T19:08:18.808-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:18.808-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T19:08:18.815-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:18.814-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:08:18.815-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:18.815-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:08:18.815-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:18.815-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:08:18.816-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:18.816-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:08:19.150-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:19.148-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:08:19.151-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:19.150-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:08:19.152-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:19.152-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:08:19.153-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:19.153-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:08:19.154-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:08:19.154-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:08:19.154-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:09:39.931-0300] {processor.py:186} INFO - Started process (PID=181572) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:09:39.931-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T19:09:39.932-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:39.932-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:09:39.948-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:09:40.024-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.024-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:09:40.033-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.033-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T19:09:40.034-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.034-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T19:09:40.035-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.034-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T19:09:40.035-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.035-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T19:09:40.043-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.043-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:09:40.044-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.044-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:09:40.045-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.044-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:09:40.045-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.045-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:09:40.501-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.499-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:09:40.502-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.501-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:09:40.503-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.502-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:09:40.504-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.503-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:09:40.504-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:09:40.504-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:09:40.505-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:10:58.377-0300] {processor.py:186} INFO - Started process (PID=182117) to work on /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:10:58.377-0300] {processor.py:914} INFO - Processing file /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py for tasks to queue
[2024-09-08T19:10:58.378-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.378-0300] {dagbag.py:587} INFO - Filling up the DagBag from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:10:58.392-0300] {processor.py:925} INFO - DAG(s) 'dataset_s3_bucket_producer_with_no_taskflow', 'dataset_alias_example_alias_consumer_with_no_taskflow', 'dataset_s3_bucket_consumer_with_no_taskflow', 'dataset_alias_example_alias_producer_with_no_taskflow' retrieved from /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
[2024-09-08T19:10:58.464-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.464-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:10:58.473-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.473-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_producer_with_no_taskflow
[2024-09-08T19:10:58.473-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.473-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_consumer_with_no_taskflow
[2024-09-08T19:10:58.473-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.473-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_s3_bucket_producer_with_no_taskflow
[2024-09-08T19:10:58.474-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.474-0300] {dag.py:3252} INFO - Creating ORM DAG for dataset_alias_example_alias_consumer_with_no_taskflow
[2024-09-08T19:10:58.480-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.480-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:10:58.481-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.481-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_alias_example_alias_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:10:58.481-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.481-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_consumer_with_no_taskflow to None, run_after=None
[2024-09-08T19:10:58.481-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.481-0300] {dag.py:4156} INFO - Setting next_dagrun for dataset_s3_bucket_producer_with_no_taskflow to None, run_after=None
[2024-09-08T19:10:58.692-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.690-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:10:58.693-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.692-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:10:58.694-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.694-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:10:58.695-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.695-0300] {dagbag.py:697} ERROR - Failed to write serialized DAG: /home/kasten/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 685, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-09-08T19:10:58.696-0300] {logging_mixin.py:190} INFO - [2024-09-08T19:10:58.696-0300] {dag.py:3229} INFO - Sync 4 DAGs
[2024-09-08T19:10:58.696-0300] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 3242, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/kasten/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_tag.name, dag_tag.dag_id
[SQL: INSERT INTO dag_tag (name, dag_id) VALUES (?, ?)]
[parameters: (('dataset-alias', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('consumer', 'dataset_alias_example_alias_consumer_with_no_taskflow'), ('producer', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset-alias', 'dataset_alias_example_alias_producer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('consumer', 'dataset_s3_bucket_consumer_with_no_taskflow'), ('dataset', 'dataset_s3_bucket_producer_with_no_taskflow'), ('producer', 'dataset_s3_bucket_producer_with_no_taskflow'))]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
